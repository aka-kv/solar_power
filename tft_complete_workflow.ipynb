{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154bfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import ocf_blosc2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import lightning.pytorch as pl\n",
    "import pytorch_forecasting as pf\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from lightning.pytorch import Trainer\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torch\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer import TemporalFusionTransformer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightning.pytorch.tuner.tuning import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwp_data = xr.open_dataset(\"../../../mnt/disks/gcp_data/nwp/ecmwf/UK_v2.zarr\")\n",
    "meta_data = pd.read_csv(\"data_files/metadata.csv\")\n",
    "pv_data = xr.open_dataset(\"data_files/pv.netcdf\", engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_ss_ids = ['8440', '16718', '8715', '17073', '9108', '9172', '10167', '10205', '10207', '10278', '26778', '26819', '10437', '10466', '26915', '10547', '26939', '26971', '10685', '10689', '2638', '2661', '2754', '2777', '2783', '2786', '2793', '2812', '2829', '2830', '2867', '2883', '2904', '2923', '2947', '2976', '2989', '2999', '3003', '3086', '3118', '3123', '3125', '3264', '3266', '3271', '3313', '3334', '3470', '3502', '11769', '11828', '11962', '3772', '11983', '3866', '3869', '4056', '4067', '4116', '4117', '4124', '4323', '4420', '20857', '4754', '13387', '13415', '5755', '5861', '5990', '6026', '6038', '6054', '14455', '6383', '6430', '6440', '6478', '6488', '6541', '6548', '6560', '14786', '6630', '6804', '6849', '6868', '6870', '6878', '6901', '6971', '7055', '7111', '7124', '7132', '7143', '7154', '7155', '7156', '7158', '7201', '7237', '7268', '7289', '7294', '7311', '7329', '7339', '7379', '7392', '7479', '7638', '7695', '7772', '15967', '7890', '16215', '7830']\n",
    "hourly_pv_data = pv_data.sel(datetime=pv_data['datetime'].dt.minute == 0)\n",
    "valid_ss_ids_data = [var for var in hourly_pv_data.data_vars if var not in skip_ss_ids]\n",
    "pv_sites_id = np.random.choice(valid_ss_ids_data, 500, replace=False)\n",
    "filtered_hourly_pv_data = hourly_pv_data[pv_sites_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_36_hour_range(start_datetime, hours=36):\n",
    "    end_datetime = start_datetime + pd.Timedelta(hours=hours - 1, minutes=59)\n",
    "    return start_datetime, end_datetime\n",
    "\n",
    "def select_non_overlapping_datetimes(datetimes, num_selections, min_gap_hours):\n",
    "    selected_datetimes = []\n",
    "    available_datetimes = list(datetimes)\n",
    "    for _ in range(num_selections):\n",
    "        if not available_datetimes:\n",
    "            break\n",
    "        random_datetime = np.random.choice(available_datetimes)\n",
    "        selected_datetimes.append(random_datetime)\n",
    "        available_datetimes = [dt for dt in available_datetimes if dt > random_datetime + pd.Timedelta(hours=min_gap_hours)]\n",
    "    return selected_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d08f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = pd.to_datetime(filtered_hourly_pv_data['datetime'].values)\n",
    "data_dict = {'ss_id': [], 'pv_datetime': [], 'generation' : [], 'horizon':[]}\n",
    "batch_size = 36\n",
    "num_selections = 5000\n",
    "min_gap_hours = 36\n",
    "\n",
    "for ss_id in pv_sites_id:\n",
    "    selected_datetimes = select_non_overlapping_datetimes(datetimes, num_selections, min_gap_hours)\n",
    "    for start_datetime in selected_datetimes:\n",
    "        start, end = get_36_hour_range(start_datetime, hours=batch_size)\n",
    "        selected_data = hourly_pv_data.sel(datetime=slice(start, end))\n",
    "        if len(selected_data['datetime']) < batch_size or selected_data[ss_id].isnull().any():\n",
    "            continue\n",
    "        hour_counter = 1\n",
    "        batch_data = {'ss_id': [], 'pv_datetime': [], 'generation': [], 'horizon': []}\n",
    "        for dt, power in zip(selected_data['datetime'].values, selected_data[ss_id].values):\n",
    "            batch_data['ss_id'].append(int(ss_id))\n",
    "            batch_data['pv_datetime'].append(dt)\n",
    "            batch_data['generation'].append(power)\n",
    "            batch_data['horizon'].append(hour_counter)\n",
    "            hour_counter += 1\n",
    "        if hour_counter - 1 == batch_size:\n",
    "            for key in data_dict.keys():\n",
    "                data_dict[key].extend(batch_data[key])\n",
    "\n",
    "pv_df = pd.DataFrame(data_dict)\n",
    "pv_df = pv_df.dropna(subset={'generation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_sites_id = [int(id) for id in pv_sites_id]\n",
    "pv_site_dict = {'ss_id':[], \"lat\":[], \"long\": [], 'tilt':[], 'orientation':[], 'kwp':[]}\n",
    "\n",
    "for id in pv_sites_id:\n",
    "    row = meta_data[meta_data['ss_id'] == id]\n",
    "    if not row.empty:\n",
    "        pv_site_dict['ss_id'].append(id)\n",
    "        pv_site_dict['lat'].append(row['latitude_rounded'].values[0])\n",
    "        pv_site_dict['long'].append(row['longitude_rounded'].values[0])\n",
    "        pv_site_dict['tilt'].append(row['tilt'].values[0])\n",
    "        pv_site_dict['orientation'].append(row['orientation'].values[0])\n",
    "        pv_site_dict['kwp'].append(row['kwp'].values[0])\n",
    "\n",
    "meta_site_df = pd.DataFrame.from_dict(pv_site_dict)\n",
    "combined_df = pd.merge(pv_df, meta_site_df, on='ss_id', how='inner')\n",
    "combined_df['pv_datetime'] = pd.to_datetime(combined_df['pv_datetime'])\n",
    "combined_df['pv_date'] = combined_df['pv_datetime'].dt.date\n",
    "combined_df['pv_hour'] = combined_df['pv_datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "batch_size = 36\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(range(0, len(combined_df), batch_size), desc=\"Processing batches\"):\n",
    "    batch = combined_df.iloc[i:i + batch_size]\n",
    "    if len(batch) < batch_size:\n",
    "        continue\n",
    "    initial_time = batch.iloc[0]['pv_datetime']\n",
    "    lat = batch.iloc[0]['lat']\n",
    "    lon = batch.iloc[0]['long']\n",
    "    nwp_sel = nwp_data.sel(latitude=lat, method=\"nearest\").sel(longitude=lon, method=\"nearest\")\n",
    "    init_time_sel = nwp_sel.sel(init_time=initial_time, method=\"ffill\")\n",
    "    if init_time_sel.init_time.size == 0:\n",
    "        continue\n",
    "    data_sel = init_time_sel.sel(step=slice(pd.Timedelta(hours=0), pd.Timedelta(hours=35)))\n",
    "    data_df = data_sel.to_dataframe().reset_index()\n",
    "    pivot_df = data_df.pivot_table(index=['init_time', 'step'], columns='variable', values='ECMWF_UK').reset_index()\n",
    "    if len(pivot_df) < batch_size:\n",
    "        continue\n",
    "    for j in range(batch_size):\n",
    "        pivot_df.loc[j, 'ss_id'] = batch.iloc[j]['ss_id']\n",
    "        pivot_df.loc[j, 'pv_datetime'] = batch.iloc[j]['pv_datetime']\n",
    "        pivot_df.loc[j, 'generation'] = batch.iloc[j]['generation']\n",
    "        pivot_df.loc[j, 'horizon'] = batch.iloc[j]['horizon']\n",
    "        pivot_df.loc[j, 'lat'] = lat\n",
    "        pivot_df.loc[j, 'long'] = lon\n",
    "        pivot_df.loc[j, 'tilt'] = batch.iloc[j]['tilt']\n",
    "        pivot_df.loc[j, 'orientation'] = batch.iloc[j]['orientation']\n",
    "        pivot_df.loc[j, 'kwp'] = batch.iloc[j]['kwp']\n",
    "        pivot_df.loc[j, 'pv_hour'] = batch.iloc[j]['pv_hour']\n",
    "    results.append(pivot_df)\n",
    "    counter += 1\n",
    "\n",
    "final_df = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d02a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_vars = ['dlwrf', 'dswrf', 'duvrs', 'sr']\n",
    "\n",
    "def cumulative_to_instantaneous(group):\n",
    "    for var in cumulative_vars:\n",
    "        group[f'{var}'] = group[var].diff().fillna(group[var])\n",
    "    return group\n",
    "\n",
    "final_df = final_df.groupby(['ss_id', 'init_time']).apply(cumulative_to_instantaneous).reset_index(drop=True)\n",
    "final_df['normalize_generation'] = final_df['generation']/final_df['kwp']\n",
    "final_df = final_df.rename(columns={'kwp': 'capacity'})\n",
    "desired_order = ['ss_id', 'init_time', 'step', 'pv_datetime', 'pv_hour', 'horizon', 'generation', 'capacity', 'normalize_generation', 'lat', 'long', 'tilt', 'orientation', 'dlwrf', 'dswrf', 'duvrs', 'hcc', 'lcc', 'mcc', 'sde', 'sr', 't2m', 'tcc', 'u10', 'u100', 'v10', 'v100']\n",
    "final_df = final_df[desired_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_per_batch = 36\n",
    "num_batches_to_keep = 100\n",
    "rows_to_keep = rows_per_batch * num_batches_to_keep\n",
    "train_data = final_df[:-rows_to_keep]\n",
    "test_data = final_df[-rows_to_keep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de29d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_data = train_data.copy()\n",
    "forecast_data = forecast_data.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "forecast_data.rename(columns={'pv_hour': 'day_hour'}, inplace=True)\n",
    "forecast_data['ss_id'] = forecast_data['ss_id'].astype(int)\n",
    "forecast_data['pv_datetime'] = pd.to_datetime(forecast_data['pv_datetime'])\n",
    "forecast_data['date'] = forecast_data['pv_datetime'].dt.date\n",
    "forecast_data['day_of_week'] = forecast_data['pv_datetime'].dt.dayofweek\n",
    "forecast_data['month'] = forecast_data['pv_datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'normalize_generation'\n",
    "static_features = ['ss_id', 'capacity', 'lat', 'long', 'tilt', 'orientation']\n",
    "known_future_inputs = ['dlwrf', 'dswrf', 'duvrs', 'hcc', 'lcc', 'mcc', 'sde', 'sr', 't2m', 'tcc', 'u10', 'u100', 'v10', 'v100', 'day_of_week', 'month', 'day_hour']\n",
    "required_columns = static_features + known_future_inputs + [target_variable, 'pv_datetime', 'date']\n",
    "forecast_data = forecast_data[required_columns]\n",
    "forecast_data = forecast_data.fillna(method='bfill').fillna(method='ffill')\n",
    "forecast_data['time_idx'] = forecast_data.index\n",
    "forecast_data['ss_id'] = forecast_data['ss_id'].astype(str)\n",
    "forecast_data['day_of_week'] = forecast_data['day_of_week'].astype(str)\n",
    "forecast_data['month'] = forecast_data['month'].astype(str)\n",
    "forecast_data['day_hour'] = forecast_data['day_hour'].astype(str)\n",
    "forecast_data['time_idx'] = forecast_data['time_idx'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 36\n",
    "max_prediction_length = 36\n",
    "training_cutoff = forecast_data[\"pv_datetime\"].max() - pd.Timedelta(hours=max_prediction_length)\n",
    "training_data = forecast_data[forecast_data[\"pv_datetime\"] <= training_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    forecast_data[lambda x: x.pv_datetime <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"normalize_generation\",\n",
    "    group_ids=[\"ss_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_reals=[\"capacity\", \"lat\", \"long\", \"tilt\", \"orientation\"],\n",
    "    time_varying_known_categoricals=[\"month\", \"day_of_week\", \"day_hour\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"dlwrf\", \"dswrf\", \"duvrs\", \"hcc\", \"lcc\", \"mcc\", \"sde\", \"sr\", \"t2m\", \"tcc\", \"u10\", \"u100\", \"v10\", \"v100\"],\n",
    "    time_varying_unknown_reals=[\"normalize_generation\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=False,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, forecast_data, min_prediction_idx=training.index.time.max() + 1, stop_randomization=True)\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eea5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-5, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7117c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer)\n",
    "res = tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=30.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96171fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdb530",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~torch.isnan(actuals) & ~torch.isnan(predictions)\n",
    "actuals_filtered = actuals[mask]\n",
    "predictions_filtered = predictions[mask]\n",
    "mae = (actuals_filtered - predictions_filtered).abs().mean().item()\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a85d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(10, len(actuals))):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actuals[i].numpy(), label='Actuals', marker='o')\n",
    "    plt.plot(predictions[i].numpy(), label='Predictions', marker='x')\n",
    "    plt.title(f'Generation Forecast vs Actual for PV site {i+1}')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Generation Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428626ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_processed = test_data.copy()\n",
    "test_data_processed.rename(columns={'pv_hour': 'day_hour'}, inplace=True)\n",
    "test_data_processed['ss_id'] = test_data_processed['ss_id'].astype(int)\n",
    "test_data_processed['pv_datetime'] = pd.to_datetime(test_data_processed['pv_datetime'])\n",
    "test_data_processed['date'] = test_data_processed['pv_datetime'].dt.date\n",
    "test_data_processed['day_of_week'] = test_data_processed['pv_datetime'].dt.dayofweek\n",
    "test_data_processed['month'] = test_data_processed['pv_datetime'].dt.month\n",
    "test_data_processed['ss_id'] = test_data_processed['ss_id'].astype(str)\n",
    "test_data_processed['day_of_week'] = test_data_processed['day_of_week'].astype(str)\n",
    "test_data_processed['month'] = test_data_processed['month'].astype(str)\n",
    "test_data_processed['day_hour'] = test_data_processed['day_hour'].astype(str)\n",
    "test_data_processed['time_idx'] = test_data_processed.index\n",
    "test_data_processed['time_idx'] = test_data_processed['time_idx'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = TimeSeriesDataSet(\n",
    "    test_data_processed,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"normalize_generation\",\n",
    "    group_ids=[\"ss_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_reals=[\"capacity\", \"lat\", \"long\", \"tilt\", \"orientation\"],\n",
    "    time_varying_known_categoricals=[\"month\", \"day_of_week\", \"day_hour\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"dlwrf\", \"dswrf\", \"duvrs\", \"hcc\", \"lcc\", \"mcc\", \"sde\", \"sr\", \"t2m\", \"tcc\", \"u10\", \"u100\", \"v10\", \"v100\"],\n",
    "    time_varying_unknown_reals=[\"normalize_generation\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=False,\n",
    ")\n",
    "\n",
    "new_data_loader = new_data.to_dataloader(train=False, batch_size=128, num_workers=2)\n",
    "test_actuals = torch.cat([y[0] for x, y in iter(new_data_loader)])\n",
    "test_predictions = best_tft.predict(new_data_loader)\n",
    "test_mae = (test_actuals - test_predictions).abs().mean().item()\n",
    "print(f\"Test Mean Absolute Error: {test_mae}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
