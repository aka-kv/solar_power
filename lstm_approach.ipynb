{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import ocf_blosc2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwp_data = xr.open_dataset(\"../../../mnt/disks/gcp_data/nwp/ecmwf/UK_v2.zarr\")\n",
    "meta_data = pd.read_csv(\"data_files/metadata.csv\")\n",
    "pv_data = xr.open_dataset(\"data_files/pv.netcdf\", engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ddb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_ss_ids = ['8440', '16718', '8715', '17073', '9108', '9172', '10167', '10205', '10207', '10278', '26778', '26819', '10437', '10466', '26915', '10547', '26939', '26971', '10685', '10689', '2638', '2661', '2754', '2777', '2783', '2786', '2793', '2812', '2829', '2830', '2867', '2883', '2904', '2923', '2947', '2976', '2989', '2999', '3003', '3086', '3118', '3123', '3125', '3264', '3266', '3271', '3313', '3334', '3470', '3502', '11769', '11828', '11962', '3772', '11983', '3866', '3869', '4056', '4067', '4116', '4117', '4124', '4323', '4420', '20857', '4754', '13387', '13415', '5755', '5861', '5990', '6026', '6038', '6054', '14455', '6383', '6430', '6440', '6478', '6488', '6541', '6548', '6560', '14786', '6630', '6804', '6849', '6868', '6870', '6878', '6901', '6971', '7055', '7111', '7124', '7132', '7143', '7154', '7155', '7156', '7158', '7201', '7237', '7268', '7289', '7294', '7311', '7329', '7339', '7379', '7392', '7479', '7638', '7695', '7772', '15967', '7890', '16215', '7830']\n",
    "hourly_pv_data = pv_data.sel(datetime=pv_data['datetime'].dt.minute == 0)\n",
    "valid_ss_ids_data = [var for var in hourly_pv_data.data_vars if var not in skip_ss_ids]\n",
    "pv_sites_id = np.random.choice(valid_ss_ids_data, 500, replace=False)\n",
    "filtered_hourly_pv_data = hourly_pv_data[pv_sites_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b51c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_36_hour_range(start_datetime, hours=36):\n",
    "    end_datetime = start_datetime + pd.Timedelta(hours=hours - 1, minutes=59)\n",
    "    return start_datetime, end_datetime\n",
    "\n",
    "def select_non_overlapping_datetimes(datetimes, num_selections, min_gap_hours):\n",
    "    selected_datetimes = []\n",
    "    available_datetimes = list(datetimes)\n",
    "    for _ in range(num_selections):\n",
    "        if not available_datetimes:\n",
    "            break\n",
    "        random_datetime = np.random.choice(available_datetimes)\n",
    "        selected_datetimes.append(random_datetime)\n",
    "        available_datetimes = [dt for dt in available_datetimes if dt > random_datetime + pd.Timedelta(hours=min_gap_hours)]\n",
    "    return selected_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6fd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = pd.to_datetime(filtered_hourly_pv_data['datetime'].values)\n",
    "data_dict = {'ss_id': [], 'pv_datetime': [], 'generation' : [], 'horizon':[]}\n",
    "batch_size = 36\n",
    "num_selections = 5000\n",
    "min_gap_hours = 36\n",
    "\n",
    "for ss_id in pv_sites_id:\n",
    "    selected_datetimes = select_non_overlapping_datetimes(datetimes, num_selections, min_gap_hours)\n",
    "    for start_datetime in selected_datetimes:\n",
    "        start, end = get_36_hour_range(start_datetime, hours=batch_size)\n",
    "        selected_data = hourly_pv_data.sel(datetime=slice(start, end))\n",
    "        if len(selected_data['datetime']) < batch_size or selected_data[ss_id].isnull().any():\n",
    "            continue\n",
    "        hour_counter = 1\n",
    "        batch_data = {'ss_id': [], 'pv_datetime': [], 'generation': [], 'horizon': []}\n",
    "        for dt, power in zip(selected_data['datetime'].values, selected_data[ss_id].values):\n",
    "            batch_data['ss_id'].append(int(ss_id))\n",
    "            batch_data['pv_datetime'].append(dt)\n",
    "            batch_data['generation'].append(power)\n",
    "            batch_data['horizon'].append(hour_counter)\n",
    "            hour_counter += 1\n",
    "        if hour_counter - 1 == batch_size:\n",
    "            for key in data_dict.keys():\n",
    "                data_dict[key].extend(batch_data[key])\n",
    "\n",
    "pv_df = pd.DataFrame(data_dict)\n",
    "pv_df = pv_df.dropna(subset={'generation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db041b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_sites_id = [int(id) for id in pv_sites_id]\n",
    "pv_site_dict = {'ss_id':[], \"lat\":[], \"long\": [], 'tilt':[], 'orientation':[], 'kwp':[]}\n",
    "\n",
    "for id in pv_sites_id:\n",
    "    row = meta_data[meta_data['ss_id'] == id]\n",
    "    if not row.empty:\n",
    "        pv_site_dict['ss_id'].append(id)\n",
    "        pv_site_dict['lat'].append(row['latitude_rounded'].values[0])\n",
    "        pv_site_dict['long'].append(row['longitude_rounded'].values[0])\n",
    "        pv_site_dict['tilt'].append(row['tilt'].values[0])\n",
    "        pv_site_dict['orientation'].append(row['orientation'].values[0])\n",
    "        pv_site_dict['kwp'].append(row['kwp'].values[0])\n",
    "\n",
    "meta_site_df = pd.DataFrame.from_dict(pv_site_dict)\n",
    "combined_df = pd.merge(pv_df, meta_site_df, on='ss_id', how='inner')\n",
    "combined_df['pv_datetime'] = pd.to_datetime(combined_df['pv_datetime'])\n",
    "combined_df['pv_date'] = combined_df['pv_datetime'].dt.date\n",
    "combined_df['pv_hour'] = combined_df['pv_datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "batch_size = 36\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(range(0, len(combined_df), batch_size), desc=\"Processing batches\"):\n",
    "    batch = combined_df.iloc[i:i + batch_size]\n",
    "    if len(batch) < batch_size:\n",
    "        continue\n",
    "    initial_time = batch.iloc[0]['pv_datetime']\n",
    "    lat = batch.iloc[0]['lat']\n",
    "    lon = batch.iloc[0]['long']\n",
    "    nwp_sel = nwp_data.sel(latitude=lat, method=\"nearest\").sel(longitude=lon, method=\"nearest\")\n",
    "    init_time_sel = nwp_sel.sel(init_time=initial_time, method=\"ffill\")\n",
    "    if init_time_sel.init_time.size == 0:\n",
    "        continue\n",
    "    data_sel = init_time_sel.sel(step=slice(pd.Timedelta(hours=0), pd.Timedelta(hours=35)))\n",
    "    data_df = data_sel.to_dataframe().reset_index()\n",
    "    pivot_df = data_df.pivot_table(index=['init_time', 'step'], columns='variable', values='ECMWF_UK').reset_index()\n",
    "    if len(pivot_df) < batch_size:\n",
    "        continue\n",
    "    for j in range(batch_size):\n",
    "        pivot_df.loc[j, 'ss_id'] = batch.iloc[j]['ss_id']\n",
    "        pivot_df.loc[j, 'pv_datetime'] = batch.iloc[j]['pv_datetime']\n",
    "        pivot_df.loc[j, 'generation'] = batch.iloc[j]['generation']\n",
    "        pivot_df.loc[j, 'horizon'] = batch.iloc[j]['horizon']\n",
    "        pivot_df.loc[j, 'lat'] = lat\n",
    "        pivot_df.loc[j, 'long'] = lon\n",
    "        pivot_df.loc[j, 'tilt'] = batch.iloc[j]['tilt']\n",
    "        pivot_df.loc[j, 'orientation'] = batch.iloc[j]['orientation']\n",
    "        pivot_df.loc[j, 'kwp'] = batch.iloc[j]['kwp']\n",
    "        pivot_df.loc[j, 'pv_hour'] = batch.iloc[j]['pv_hour']\n",
    "    results.append(pivot_df)\n",
    "    counter += 1\n",
    "\n",
    "final_df = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_vars = ['dlwrf', 'dswrf', 'duvrs', 'sr']\n",
    "\n",
    "def cumulative_to_instantaneous(group):\n",
    "    for var in cumulative_vars:\n",
    "        group[f'{var}'] = group[var].diff().fillna(group[var])\n",
    "    return group\n",
    "\n",
    "final_df = final_df.groupby(['ss_id', 'init_time']).apply(cumulative_to_instantaneous).reset_index(drop=True)\n",
    "final_df['normalize_generation'] = final_df['generation']/final_df['kwp']\n",
    "final_df = final_df.rename(columns={'kwp': 'capacity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af45774",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_per_batch = 36\n",
    "num_batches_to_keep = 100\n",
    "rows_to_keep = rows_per_batch * num_batches_to_keep\n",
    "train_data = final_df[:-rows_to_keep]\n",
    "test_data = final_df[-rows_to_keep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffed03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=36):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        weather_cols = ['dlwrf', 'dswrf', 'duvrs', 'hcc', 'lcc', 'mcc', 'sde', 'sr', 't2m', 'tcc', 'u10', 'u100', 'v10', 'v100']\n",
    "        static_cols = ['capacity', 'lat', 'long', 'tilt', 'orientation']\n",
    "        self.feature_cols = weather_cols + static_cols\n",
    "        self.target_col = 'normalize_generation'\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        self.X = self.scaler_X.fit_transform(data[self.feature_cols].values)\n",
    "        self.y = self.scaler_y.fit_transform(data[[self.target_col]].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_length]\n",
    "        y_val = self.y[idx+self.seq_length]\n",
    "        return torch.FloatTensor(X_seq), torch.FloatTensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f9fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SolarDataset(train_data, seq_length=36)\n",
    "test_dataset = SolarDataset(test_data, seq_length=36)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = len(train_dataset.feature_cols)\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "output_size = 1\n",
    "model = LSTMForecaster(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983abb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(test_loader))\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eafad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('LSTM Training History')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(y_batch.cpu().numpy())\n",
    "\n",
    "predictions = test_dataset.scaler_y.inverse_transform(np.array(predictions))\n",
    "actuals = test_dataset.scaler_y.inverse_transform(np.array(actuals))\n",
    "\n",
    "mae = np.mean(np.abs(predictions - actuals))\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ba181",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals[:100], label='Actual', marker='o')\n",
    "plt.plot(predictions[:100], label='Predicted', marker='x')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Normalized Generation')\n",
    "plt.title('LSTM: Actual vs Predicted Solar Generation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
